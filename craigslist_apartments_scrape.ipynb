{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1 scraped successfully!\n",
      "Page 2 scraped successfully!\n",
      "Page 3 scraped successfully!\n",
      "Page 4 scraped successfully!\n",
      "Page 5 scraped successfully!\n",
      "Page 6 scraped successfully!\n",
      "Page 7 scraped successfully!\n",
      "Page 8 scraped successfully!\n",
      "Page 9 scraped successfully!\n",
      "Page 10 scraped successfully!\n",
      "Page 11 scraped successfully!\n",
      "Page 12 scraped successfully!\n",
      "Page 13 scraped successfully!\n",
      "Page 14 scraped successfully!\n",
      "Page 15 scraped successfully!\n",
      "Page 16 scraped successfully!\n",
      "Page 17 scraped successfully!\n",
      "Page 18 scraped successfully!\n",
      "Page 19 scraped successfully!\n",
      "Page 20 scraped successfully!\n",
      "Page 21 scraped successfully!\n",
      "Page 22 scraped successfully!\n",
      "Page 23 scraped successfully!\n",
      "Page 24 scraped successfully!\n",
      "Page 25 scraped successfully!\n",
      "Page 26 scraped successfully!\n",
      "\n",
      "\n",
      "Scrape complete!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from IPython.core.display import clear_output\n",
    "from geopy.geocoders import Nominatim\n",
    "from warnings import warn\n",
    "\n",
    "citystr = 'minneapolis' \n",
    "linkpfx = 'https://'+citystr+'.craigslist.org/search/apa?'\n",
    "linksfx =( 'sort=date'\n",
    "          +'&hasPic=1'\n",
    "          +'&bundleDuplicates=1'\n",
    "          +'&search_distance=7'\n",
    "          +'&postal=55454' \n",
    "          +'&min_price=300'\n",
    "          +'&max_price=8000'\n",
    "          +'&max_bedrooms=4'\n",
    "          +'&availabilityMode=0'\n",
    "          +'&sale_date=all+dates')\n",
    "response = requests.get(linkpfx+linksfx)\n",
    "html_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "results_num = html_soup.find('div', class_= 'search-legend')\n",
    "results_total = int(results_num.find('span', class_='totalcount').text) #pulled the total count of posts as the upper bound of the pages array\n",
    "pages = np.arange(0, results_total+1, 120)\n",
    "\n",
    "geolocator = Nominatim(user_agent=\"asdfasdfasdf\",timeout=5)\n",
    "\n",
    "#\n",
    "TypeDict = {\n",
    "    'apartment': 1, \n",
    "    'condo':2, \n",
    "    'cottage/cabin':3, \n",
    "    'duplex':4, \n",
    "    'flat':5, \n",
    "    'house':6, \n",
    "    'in-law':7, \n",
    "    'loft':8, \n",
    "    'townhouse':9, \n",
    "    'manufactured':10, \n",
    "    'assisted living':11, \n",
    "    'land':12\n",
    "}\n",
    "LaundryDict = {\n",
    "    'w/d in unit':1,\n",
    "    'w/d hookups':2,\n",
    "    'laundry in bldg':3,\n",
    "    'laundry on site':4,\n",
    "    'no laundry on site':5\n",
    "}    \n",
    "ParkingDict = {\n",
    "    'carport':1,\n",
    "    'attached garage':2,\n",
    "    'detached garage':3,\n",
    "    'off-street parking':4,\n",
    "    'street parking':5,\n",
    "    'valet parking':6,\n",
    "    'no parking':7\n",
    "}\n",
    "\n",
    "iterations = 0\n",
    "\n",
    "post_timing = []\n",
    "post_hoods = []\n",
    "post_title_texts = []\n",
    "bedroom_counts = []\n",
    "sqfts = []\n",
    "post_links = []\n",
    "post_prices = []\n",
    "\n",
    "cities = []\n",
    "towns = []\n",
    "neighborhoods = []\n",
    "zips = []\n",
    "\n",
    "types = []\n",
    "laundries = []\n",
    "parkings = []\n",
    "\n",
    "for page in pages:\n",
    "    \n",
    "    #get request\n",
    "    response = requests.get(linkpfx\n",
    "                   + 's=' #parameter for defining page number\n",
    "                   + str(page) #page in above array\n",
    "                   + linksfx)    \n",
    "     \n",
    "    #throw warning for status codes that are not 200\n",
    "    if response.status_code != 200:\n",
    "        warn('Request: {}; Status code: {}'.format(requests, response.status_code))\n",
    "        \n",
    "    #define the html text\n",
    "    page_html = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    #define the posts\n",
    "    posts = html_soup.find_all('li', class_= 'result-row')\n",
    "        \n",
    "    #extract data item-wise\n",
    "    for post in posts:\n",
    "        \n",
    "        curpost = requests.get(post.find('a', class_='result-title hdrlnk')['href']).text\n",
    "        curmap = BeautifulSoup(curpost,'html.parser').find_all('div',class_='mapbox')\n",
    "        \n",
    "        if (post.find('span', class_ = 'result-hood') is not None) and curmap and (\n",
    "        post.find('a', class_='result-title hdrlnk')['href'] not in post_links):\n",
    "\n",
    "            #posting date\n",
    "            post_datetime = post.find('time', class_= 'result-date')['datetime']\n",
    "            post_timing.append(post_datetime)\n",
    "\n",
    "            #neighborhoods\n",
    "            post_hood = post.find('span', class_= 'result-hood').text\n",
    "            post_hoods.append(post_hood)\n",
    "\n",
    "            #title text\n",
    "            post_title = post.find('a', class_='result-title hdrlnk')\n",
    "            post_title_text = post_title.text\n",
    "            post_title_texts.append(post_title_text)\n",
    "\n",
    "            #post link\n",
    "            post_link = post_title['href']\n",
    "            post_links.append(post_link)\n",
    "            \n",
    "            #run geolocator on the latitude/longitude \n",
    "            curmap = curmap[0] \n",
    "            location = geolocator.reverse(curmap.find('div')['data-latitude']+\",\"+curmap.find('div')['data-longitude'])\n",
    "            \n",
    "            try: \n",
    "                cities.append(location.raw['address']['city'])\n",
    "            except:\n",
    "                try: \n",
    "                    cities.append(location.raw['address']['town'])\n",
    "                except:\n",
    "                    cities.append('')\n",
    "            \n",
    "            try: \n",
    "                towns.append(location.raw['address']['town'])\n",
    "            except:\n",
    "                towns.append('')\n",
    "            \n",
    "            try:\n",
    "                neighborhoods.append(location.raw['address']['neighbourhood'])\n",
    "            except:\n",
    "                try: \n",
    "                    locshift1 = geolocator.reverse( str(float(curmap.find('div')['data-latitude'])+.001)+\",\"+str(float(curmap.find('div')['data-longitude'])+.001))\n",
    "                    neighborhoods.append(locshift1.raw['address']['neighbourhood'])\n",
    "                except:                   \n",
    "                    try: \n",
    "                        locshift2 = geolocator.reverse( str(float(curmap.find('div')['data-latitude'])-.001)+\",\"+str(float(curmap.find('div')['data-longitude'])-.001))\n",
    "                        neighborhoods.append(locshift2.raw['address']['neighbourhood'])\n",
    "                    except:\n",
    "                        try:\n",
    "                            neighborhoods.append(location.raw['address']['town'])\n",
    "                        except:\n",
    "                            try:\n",
    "                                neighborhoods.append(location.raw['address']['city'])\n",
    "                            except:\n",
    "                                neighborhoods.append('')\n",
    "            \n",
    "            try:\n",
    "                zips.append(location.raw['address']['postcode'])\n",
    "            except:\n",
    "                zips.append('')\n",
    "            \n",
    "            try:\n",
    "                curattr = BeautifulSoup(curpost,'html.parser').find_all('p',class_='attrgroup')[1]\n",
    "                typename = list(set(curattr.text.split('\\n')).intersection(list(TypeDict.keys())))[0]\n",
    "                types.append(TypeDict[typename])\n",
    "            except:\n",
    "                types.append('')\n",
    "            \n",
    "            try:\n",
    "                curattr = BeautifulSoup(curpost,'html.parser').find_all('p',class_='attrgroup')[1]\n",
    "                laundryname = list(set(curattr.text.split('\\n')).intersection(list(LaundryDict.keys())))[0]\n",
    "                laundries.append(LaundryDict[laundryname])\n",
    "            except:\n",
    "                laundries.append('')\n",
    "            \n",
    "            try:\n",
    "                curattr = BeautifulSoup(curpost,'html.parser').find_all('p',class_='attrgroup')[1]\n",
    "                parkname = list(set(curattr.text.split('\\n')).intersection(list(ParkingDict.keys())))[0]\n",
    "                parkings.append(ParkingDict[parkname])\n",
    "            except:\n",
    "                parkings.append('')\n",
    "            \n",
    "            #removes the \\n whitespace from each side, removes the currency symbol, and turns it into an int\n",
    "            post_price = int(post.a.text.strip().replace(\"$\", \"\")) \n",
    "            post_prices.append(post_price)\n",
    "            \n",
    "            if post.find('span', class_ = 'housing') is not None:\n",
    "                \n",
    "                #if the first element is accidentally square footage\n",
    "                if 'ft2' in post.find('span', class_ = 'housing').text.split()[0]:\n",
    "                    \n",
    "                    #make bedroom nan\n",
    "                    bedroom_count = np.nan\n",
    "                    bedroom_counts.append(bedroom_count)\n",
    "                    \n",
    "                    #make sqft the first element\n",
    "                    sqft = int(post.find('span', class_ = 'housing').text.split()[0][:-3])\n",
    "                    sqfts.append(sqft)\n",
    "                    \n",
    "                #if the length of the housing details element is more than 2\n",
    "                elif len(post.find('span', class_ = 'housing').text.split()) > 2:\n",
    "                    \n",
    "                    #therefore element 0 will be bedroom count\n",
    "                    bedroom_count = post.find('span', class_ = 'housing').text.replace(\"br\", \"\").split()[0]\n",
    "                    bedroom_counts.append(bedroom_count)\n",
    "                    \n",
    "                    #and sqft will be number 3, so set these here and append\n",
    "                    sqft = int(post.find('span', class_ = 'housing').text.split()[2][:-3])\n",
    "                    sqfts.append(sqft)\n",
    "                    \n",
    "                #if there is num bedrooms but no sqft\n",
    "                elif len(post.find('span', class_ = 'housing').text.split()) == 2:\n",
    "                    \n",
    "                    #therefore element 0 will be bedroom count\n",
    "                    bedroom_count = post.find('span', class_ = 'housing').text.replace(\"br\", \"\").split()[0]\n",
    "                    bedroom_counts.append(bedroom_count)\n",
    "                    \n",
    "                    #and sqft will be number 3, so set these here and append\n",
    "                    sqft = np.nan\n",
    "                    sqfts.append(sqft)                    \n",
    "                \n",
    "                else:\n",
    "                    bedroom_count = np.nan\n",
    "                    bedroom_counts.append(bedroom_count)\n",
    "                \n",
    "                    sqft = np.nan\n",
    "                    sqfts.append(sqft)\n",
    "                \n",
    "            #if none of those conditions catch, make bedroom nan, this won't be needed    \n",
    "            else:\n",
    "                bedroom_count = np.nan\n",
    "                bedroom_counts.append(bedroom_count)\n",
    "                \n",
    "                sqft = np.nan\n",
    "                sqfts.append(sqft)\n",
    "            \n",
    "    iterations += 1\n",
    "    print(\"Page \" + str(iterations) + \" scraped successfully!\")\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Scrape complete!\")\n",
    "\n",
    "\n",
    "\n",
    "apts = pd.DataFrame({\n",
    "    'posted': post_timing,\n",
    "    'neighborhood': post_hoods,\n",
    "    'post title': post_title_texts,\n",
    "    'number bedrooms': bedroom_counts,\n",
    "    'sqft': sqfts,\n",
    "    'URL': post_links,\n",
    "    'price': post_prices,\n",
    "    'cities':cities,\n",
    "    'towns':towns,\n",
    "    'neighborhoods':neighborhoods,\n",
    "    'zips': zips,\n",
    "    'types':types,\n",
    "    'parking':parkings,\n",
    "    'laundry':laundries})\n",
    "\n",
    "#make the number bedrooms to a float (since np.nan is a float too)\n",
    "apts['number bedrooms'] = apts['number bedrooms'].apply(lambda x: float(x))\n",
    "\n",
    "#convert datetime string into datetime object to be able to work with it\n",
    "apts['posted'] = pd.to_datetime(apts['posted'])\n",
    "\n",
    "#write to directory\n",
    "apts.to_csv(path_or_buf='./scraped_data/apts_'+citystr+\"_\"+str(datetime.now()).replace(' ','_').replace(':','-')+'.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
