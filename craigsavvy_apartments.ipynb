{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bs4.element.ResultSet'>\n",
      "120\n"
     ]
    }
   ],
   "source": [
    "#import get to call a get request on the site\n",
    "from requests import get\n",
    "\n",
    "#get the first page of the housing prices\n",
    "response = get('https://minneapolis.craigslist.org/search/apa?hasPic=1&availabilityMode=0') #get rid of those lame-o's that post a housing option without a pic using their filter\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "html_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "#get the macro-container for the housing posts\n",
    "posts = html_soup.find_all('li', class_= 'result-row')\n",
    "print(type(posts)) #to double check that I got a ResultSet\n",
    "print( len(posts)) #to double check I got 120 (elements/page)\n",
    "post = posts[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First post: $1296\n",
      "\n",
      "\n",
      "\n",
      "favorite this post\n",
      "\n",
      "May 28\n",
      "Third Floor, Neat Vaulted Ceiling, Available August!\n",
      "\n",
      "$1296\n",
      "\n",
      "                    2br -\n",
      "                    990ft2 -\n",
      "                \n",
      " (Burnsville)\n",
      "\n",
      "pic\n",
      "\n",
      "\n",
      "hide this posting\n",
      "\n",
      "\n",
      "\n",
      "restore\n",
      "restore this posting\n"
     ]
    }
   ],
   "source": [
    "print(\"First post: \" + post.text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-28 12:25\n",
      "https://minneapolis.craigslist.org/dak/apa/d/burnsville-third-floor-neat-vaulted/7131845270.html\n"
     ]
    }
   ],
   "source": [
    "curtime = post.find('time',class_='result-date')['datetime']\n",
    "print(curtime)\n",
    "cururl = post.find('a',class_='result-title hdrlnk')['href']\n",
    "print(cururl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "990\n",
      " (Burnsville)\n"
     ]
    }
   ],
   "source": [
    "curbrs = post.find('span', class_ = 'housing').text.split()[0][:-2]\n",
    "print(curbrs)\n",
    "\n",
    "cursqft = post.find('span', class_ = 'housing').text.split()[2][:-3] #cleans the ft2 at the end\n",
    "print(cursqft)\n",
    "\n",
    "\n",
    "curhood = post.find('span', class_='result-hood') #grabs the neighborhood, this is the problem column that requires\n",
    "#a lot of cleaning and figuring out later.\n",
    "if curhood:\n",
    "    curhood = curhood.text\n",
    "    print(curhood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1 scraped successfully!\n",
      "Page 2 scraped successfully!\n",
      "Page 3 scraped successfully!\n",
      "Page 4 scraped successfully!\n",
      "Page 5 scraped successfully!\n",
      "Page 6 scraped successfully!\n",
      "Page 7 scraped successfully!\n",
      "Page 8 scraped successfully!\n",
      "Page 9 scraped successfully!\n",
      "Page 10 scraped successfully!\n",
      "Page 11 scraped successfully!\n",
      "Page 12 scraped successfully!\n",
      "Page 13 scraped successfully!\n",
      "Page 14 scraped successfully!\n",
      "Page 15 scraped successfully!\n",
      "Page 16 scraped successfully!\n",
      "Page 17 scraped successfully!\n",
      "Page 18 scraped successfully!\n",
      "Page 19 scraped successfully!\n",
      "Page 20 scraped successfully!\n",
      "Page 21 scraped successfully!\n",
      "Page 22 scraped successfully!\n",
      "Page 23 scraped successfully!\n",
      "Page 24 scraped successfully!\n",
      "Page 25 scraped successfully!\n",
      "Page 26 scraped successfully!\n",
      "\n",
      "\n",
      "Scrape complete!\n"
     ]
    }
   ],
   "source": [
    "#build out the loop\n",
    "from time import sleep, time\n",
    "import re\n",
    "from random import randint #avoid throttling by not sending too many requests one after the other\n",
    "from warnings import warn\n",
    "from IPython.core.display import clear_output\n",
    "import numpy as np\n",
    "\n",
    "#find the total number of posts to find the limit of the pagination\n",
    "results_num = html_soup.find('div', class_= 'search-legend')\n",
    "results_total = int(results_num.find('span', class_='totalcount').text) #pulled the total count of posts as the upper bound of the pages array\n",
    "\n",
    "#each page has 119 posts so each new page is defined as follows: s=120, s=240, s=360, and so on. So we need to step in size 120 in the np.arange function\n",
    "pages = np.arange(0, results_total+1, 120)\n",
    "\n",
    "iterations = 0\n",
    "\n",
    "post_timing = []\n",
    "post_hoods = []\n",
    "post_title_texts = []\n",
    "bedroom_counts = []\n",
    "sqfts = []\n",
    "post_links = []\n",
    "post_prices = []\n",
    "\n",
    "for page in pages:\n",
    "    \n",
    "    #get request\n",
    "    response = get('https://minneapolis.craigslist.org/search/apa?'\n",
    "                   + 's=' #parameter for defining page number\n",
    "                   + str(page) #page in above array\n",
    "                   + 'hasPic=1'\n",
    "                   + '&availabilityMode=0')    \n",
    "    sleep(randint(1,5))\n",
    "     \n",
    "    #throw warning for status codes that are not 200\n",
    "    if response.status_code != 200:\n",
    "        warn('Request: {}; Status code: {}'.format(requests, response.status_code))\n",
    "        \n",
    "    #define the html text\n",
    "    page_html = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    #define the posts\n",
    "    posts = html_soup.find_all('li', class_= 'result-row')\n",
    "        \n",
    "    #extract data item-wise\n",
    "    for post in posts:\n",
    "\n",
    "        if post.find('span', class_ = 'result-hood') is not None:\n",
    "\n",
    "            #posting date\n",
    "            #grab the datetime element 0 for date and 1 for time\n",
    "            post_datetime = post.find('time', class_= 'result-date')['datetime']\n",
    "            post_timing.append(post_datetime)\n",
    "\n",
    "            #neighborhoods\n",
    "            post_hood = post.find('span', class_= 'result-hood').text\n",
    "            post_hoods.append(post_hood)\n",
    "\n",
    "            #title text\n",
    "            post_title = post.find('a', class_='result-title hdrlnk')\n",
    "            post_title_text = post_title.text\n",
    "            post_title_texts.append(post_title_text)\n",
    "\n",
    "            #post link\n",
    "            post_link = post_title['href']\n",
    "            post_links.append(post_link)\n",
    "            \n",
    "            #removes the \\n whitespace from each side, removes the currency symbol, and turns it into an int\n",
    "            post_price = int(post.a.text.strip().replace(\"$\", \"\")) \n",
    "            post_prices.append(post_price)\n",
    "            \n",
    "            if post.find('span', class_ = 'housing') is not None:\n",
    "                \n",
    "                #if the first element is accidentally square footage\n",
    "                if 'ft2' in post.find('span', class_ = 'housing').text.split()[0]:\n",
    "                    \n",
    "                    #make bedroom nan\n",
    "                    bedroom_count = np.nan\n",
    "                    bedroom_counts.append(bedroom_count)\n",
    "                    \n",
    "                    #make sqft the first element\n",
    "                    sqft = int(post.find('span', class_ = 'housing').text.split()[0][:-3])\n",
    "                    sqfts.append(sqft)\n",
    "                    \n",
    "                #if the length of the housing details element is more than 2\n",
    "                elif len(post.find('span', class_ = 'housing').text.split()) > 2:\n",
    "                    \n",
    "                    #therefore element 0 will be bedroom count\n",
    "                    bedroom_count = post.find('span', class_ = 'housing').text.replace(\"br\", \"\").split()[0]\n",
    "                    bedroom_counts.append(bedroom_count)\n",
    "                    \n",
    "                    #and sqft will be number 3, so set these here and append\n",
    "                    sqft = int(post.find('span', class_ = 'housing').text.split()[2][:-3])\n",
    "                    sqfts.append(sqft)\n",
    "                    \n",
    "                #if there is num bedrooms but no sqft\n",
    "                elif len(post.find('span', class_ = 'housing').text.split()) == 2:\n",
    "                    \n",
    "                    #therefore element 0 will be bedroom count\n",
    "                    bedroom_count = post.find('span', class_ = 'housing').text.replace(\"br\", \"\").split()[0]\n",
    "                    bedroom_counts.append(bedroom_count)\n",
    "                    \n",
    "                    #and sqft will be number 3, so set these here and append\n",
    "                    sqft = np.nan\n",
    "                    sqfts.append(sqft)                    \n",
    "                \n",
    "                else:\n",
    "                    bedroom_count = np.nan\n",
    "                    bedroom_counts.append(bedroom_count)\n",
    "                \n",
    "                    sqft = np.nan\n",
    "                    sqfts.append(sqft)\n",
    "                \n",
    "            #if none of those conditions catch, make bedroom nan, this won't be needed    \n",
    "            else:\n",
    "                bedroom_count = np.nan\n",
    "                bedroom_counts.append(bedroom_count)\n",
    "                \n",
    "                sqft = np.nan\n",
    "                sqfts.append(sqft)\n",
    "            #    bedroom_counts.append(bedroom_count)\n",
    "                \n",
    "            #    sqft = np.nan\n",
    "            #    sqfts.append(sqft)\n",
    "                \n",
    "    iterations += 1\n",
    "    print(\"Page \" + str(iterations) + \" scraped successfully!\")\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Scrape complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: C:\\Users\\scott\\Miniconda3\\envs\\craigsavvy\n",
      "\n",
      "  added / updated specs:\n",
      "    - pandas\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    pandas-1.0.3               |   py37h47e9c7a_0         7.4 MB\n",
      "    pytz-2020.1                |             py_0         184 KB\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         7.6 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  pandas             pkgs/main/win-64::pandas-1.0.3-py37h47e9c7a_0\n",
      "  pytz               pkgs/main/noarch::pytz-2020.1-py_0\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "\n",
      "pandas-1.0.3         | 7.4 MB    |            |   0% \n",
      "pandas-1.0.3         | 7.4 MB    |            |   0% \n",
      "pandas-1.0.3         | 7.4 MB    | #1         |  12% \n",
      "pandas-1.0.3         | 7.4 MB    | ##2        |  22% \n",
      "pandas-1.0.3         | 7.4 MB    | ###4       |  35% \n",
      "pandas-1.0.3         | 7.4 MB    | ####5      |  46% \n",
      "pandas-1.0.3         | 7.4 MB    | #####5     |  55% \n",
      "pandas-1.0.3         | 7.4 MB    | ######4    |  65% \n",
      "pandas-1.0.3         | 7.4 MB    | #######6   |  77% \n",
      "pandas-1.0.3         | 7.4 MB    | ########8  |  88% \n",
      "pandas-1.0.3         | 7.4 MB    | ########## | 100% \n",
      "\n",
      "pytz-2020.1          | 184 KB    |            |   0% \n",
      "pytz-2020.1          | 184 KB    | ########## | 100% \n",
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... done\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-afd979800859>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m eb_apts = pd.DataFrame({'posted': post_timing,\n\u001b[0;32m      4\u001b[0m                        \u001b[1;34m'neighborhood'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpost_hoods\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                        \u001b[1;34m'post title'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpost_title_texts\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "eb_apts = pd.DataFrame({'posted': post_timing,\n",
    "                       'neighborhood': post_hoods,\n",
    "                       'post title': post_title_texts,\n",
    "                       'number bedrooms': bedroom_counts,\n",
    "                        'sqft': sqfts,\n",
    "                        'URL': post_links,\n",
    "                       'price': post_prices})\n",
    "print(eb_apts.info())\n",
    "eb_apts.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
